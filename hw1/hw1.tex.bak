
\documentclass[10pt]{article}

\usepackage{graphicx}
\usepackage{amsmath,amsfonts,amssymb,epstopdf}

\usepackage{hyperref}  % for urls and hyperlinks
\newcommand\tab[1][0.5cm]{\hspace*{#1}}

\setlength{\textwidth}{6.2in}
\setlength{\oddsidemargin}{0.3in}
\setlength{\evensidemargin}{0in}
\setlength{\textheight}{8.9in}
\setlength{\voffset}{-1in}
\setlength{\headsep}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt}


\input{../latex/macros.tex}


\begin{document}

% header:
\hfill \vbox{
\hbox{AMath 584 / Math 584}
\hbox{Homework \#1}
\hbox{Due 11:00pm PDT}
\hbox{Tuesday, October 11, 2016}
}


\vskip 0.5cm

{\bf Name:}   Yifei Guan

{\bf Netid:}  gyf135

\vskip 0.5cm

%--------------------------------------------------------------------------
\vskip 1cm
\hrule
{\bf Problem 1.}
Exercise 1.1 in Trefethen and Bau.
Note for example that part (a) is accomplished by multiplying $B$ on the
right by the matrix
\[
C_1 = \bcm 2&0&0&0\\ 0&1&0&0\\ 0&0&1&0\\ 0&0&0&1\ecm
\]
You can write out analogous matrices for each step and then write the
required product of these matrices.  Make sure to write them
{\em in the correct order.}

% If you are writing solutions in latex, note that the macros \bcm and \ecm
% stand for begin centered matrix and "end centered matrix" and are defined
% in ../latex/macros.tex.  This centers the matrix entries.
% Also useful sometimes are \brm and \erm for right-justified entries, e.g.
%  $\brm -1&0\\ 0&1\erm$ looks nicer than with centered entries.

% uncomment the next two lines if you want to insert solution...
\vskip 1cm
{\bf Solution:}\\
% insert your solution here!
Exercise 1.1 (a)
\[\bcm 1&-1&0&0\\ 0&1&0&0\\ 0&-1&1&0\\ 0&-1&0&1\ecm
\bcm 1&0&1&0\\ 0&1&0&0\\ 0&0&1&0\\ 0&0&0&1\ecm
\bcm 1&0&0&0\\ 0&1&0&0\\ 0&0&\frac{1}{2}&0\\ 0&0&0&1\ecm
B
\bcm 2&0&0&0\\ 0&1&0&0\\ 0&0&1&0\\ 0&0&0&1\ecm
\bcm 0&0&0&1\\ 0&1&0&0\\ 0&0&1&0\\ 1&0&0&0\ecm\] \\
\[\bcm 1&0&0&0\\ 0&1&0&0\\ 0&0&1&1\\ 0&0&0&0\ecm
\bcm 0&0&0\\ 1&0&0\\ 0&1&0\\ 0&0&1\ecm\]

Exercise 1.1 (b)
\[
A = \bcm 1&-1&0&0\\ 0&1&0&0\\ 0&-1&1&0\\ 0&-1&0&1\ecm
\bcm 1&0&1&0\\ 0&1&0&0\\ 0&0&1&0\\ 0&0&0&1\ecm
\bcm 1&0&0&0\\ 0&1&0&0\\ 0&0&\frac{1}{2}&0\\ 0&0&0&1\ecm
= \bcm 1&-1&\frac{1}{2}&0\\ 0&1&0&0\\ 0&-1&\frac{1}{2}&0\\ 0&-1&0&1\ecm
\] \\
\[
 C = \bcm 2&0&0&0\\ 0&1&0&0\\ 0&0&1&0\\ 0&0&0&1\ecm
\bcm 0&0&0&1\\ 0&1&0&0\\ 0&0&1&0\\ 1&0&0&0\ecm
\bcm 1&0&0&0\\ 0&1&0&0\\ 0&0&1&1\\ 0&0&0&0\ecm
\bcm 0&0&0\\ 1&0&0\\ 0&1&0\\ 0&0&1\ecm
= \bcm 0&0&0\\ 1&0&0\\ 0&1&1\\ 0&0&0\ecm\]\\
\[
ABC = \bcm 1&-1&\frac{1}{2}&0\\ 0&1&0&0\\ 0&-1&\frac{1}{2}&0\\ 0&-1&0&1\ecm
B
\bcm 0&0&0\\ 1&0&0\\ 0&1&1\\ 0&0&0\ecm\]
%--------------------------------------------------------------------------
\vskip 1cm
\hrule
{\bf Problem 2.}
Exercise 2.1 in Trefethen and Bau.


% uncomment the next two lines if you want to insert solution...

% insert your solution here!

\vskip 1cm
{\bf Solution:}\\
Exercise 2.1\\
Suppose A is a $m \times m$ upper triangular and unitary matrix
then $a_i,_j = 0$ for $i > j$ and $A^* = A^{-1}$
Let $C = A^{-1}$ then $e_j = \sum\limits_{i=1}^{m} c_i,_j a_i$ where $e_i$ denotes each column vector of
a $m \times m$ identity matrix. Then $e_j = Ac_j$ and since $a_i,_j = 0$ for $i < j$\\
\[\bcm a_1,_1& \cdots &\cdots &a_1,_m\\ 0&a_2,_2& \cdots &a_2,_m\\ 0&0& \cdots &\cdots\\ 0& \cdots &a_{m-1},_{m-1}&a_{m-1},_m \\ 0& \cdots &0&a_m,_m\ecm
\bcm c_1,_j\\c_2,_j\\ \vdots \\c_{m-1},_j\\c_m,_j\ecm
= \bcm 0\\ \vdots \\e_j,_j\neq 0\\ \vdots \\0\ecm
\]\\
Since from the last row and induce upwards, we can find that $c_k,_j = 0$ for all $ k>j $. Therefore, C is a upper triangular matrix. And also $ A^* = A^{-1} = C $ is upper triangular. Since matrix A is also upper triangular, $ A^* $ is both upper and lower triangular. Therefore, $ A^* $ and A are both diagonal.\\
If A is lower triangular matrix, the proof will be similar.
%--------------------------------------------------------------------------
\vskip 1cm
\hrule
{\bf Problem 3.}
Exercise 2.6 in Trefethen and Bau.  In addition, please note the value that
$\alpha$ takes in the special case when $u$ and $v$ are orthogonal.


% uncomment the next two lines if you want to insert solution...
\vskip 1cm
{\bf Solution:}\\
% insert your solution here!
Exercise 2.6\\
If A is nonsingular, write $A^{-1}$ in column form such as $A^{-1} = \bcm x_1,\cdots,x_m\ecm $ where $ x_1 \cdots x_m$ are column vectors. Then $AA^{-1} = (I + uv^*)\bcm x_1,\cdots,x_m\ecm = \bcm x_1+uv^*x_1,\cdots, x_m+uv^*x_m \ecm = I = \bcm e_1, \cdots, e_m \ecm$. Hence, $x_i+uv^*x_i=e_i$. Note that $v^*x_i$ is just some constant. We therefore denote $c_i = v^*x_i$ and thus $x_i = e_i - c_iu$ and $A^{-1} = I - uC$ where $C = \bcm c_1\\ \cdots\\ c_m \ecm$.\\
Therefore,\\
\centerline {$I = AA^{-1} = (I+uv^*)(I-uC) = I - uC + uv^* - uv^*uC$}\\
which implies that $- uC_i + uv_i - u(v^*u)C_i = 0$. Solving for $C_i$ gives $C_i = \frac{v_i}{1+v^*u}$, and then\\
\centerline {$A^{-1} = I - uC = I + \alpha uv^*$ where $\alpha = -\frac{1}{1+v^*u}$}\\

Since $det|A| = 0$, suppose $ \exists x \in \mathbb{C}^m$ such that $Ax = x + uv^*x = 0 \Rightarrow x = -u(v^*x)$. Note that $v^*x$ is just some constant. We denote that $x = \beta u for \beta = v^*x \in \mathbb{C}$. Then the equation $Ax=0$ is equivalent to $\beta u + u(v^*(\beta u)) = \beta u(1+v^*u) = 0$.\\
Therefore, for A to be singular $v^*u = -1$.\\
On the converse, suppose $v^*u = -1$, then $\forall \beta \in \mathbb{C}, A(\beta u) = \beta u(1+v^*u) = 0$, which implies A is singular. Therefor, A is singular if and only if $v^*u = -1$. In this case, $null(A) = \beta u,  \forall \beta \in \mathbb{C}$.
%--------------------------------------------------------------------------
\vskip 1cm
\hrule
{\bf Problem 4.}
Exercises 3.1, 3.3, and 3.4 in Trefethen and Bau.


% uncomment the next two lines if you want to insert solution...
\vskip 1cm
{\bf Solution:}\\
% insert your solution here!
Exercise 3.1\\
To verify condition 1:\\
\centerline {$\parallel x\parallel_W = \parallel Wx\parallel = (\sum\limits_{i=1}^{m} | \sum\limits_{j=1}^{n} w_i,_jx_j|^p)^{1/p} \geq 0$} \\
The only way $\parallel x\parallel_W = 0$ is $Wx = 0$. Since $W$ is non-singular, $\parallel x\parallel_W = 0$ only if $x = 0$.\\

To verify condition 2:\\
\centerline {$\parallel x+y\parallel_W = \parallel W(x+y)\parallel = \parallel Wx+Wy\parallel \leq \parallel Wx\parallel + \parallel Wy\parallel = \parallel x\parallel_W + \parallel y\parallel_W$}\\

To verify condition 3:\\
\centerline {$\parallel\alpha x\parallel_W = \parallel W(\alpha x)\parallel = \parallel\alpha Wx\parallel = |\alpha| \parallel Wx\parallel = |\alpha| \parallel x\parallel_W$}\\
Therefore, for an arbitrary non-singular matrix $W$, $\parallel x \parallel_W$ defined by $\parallel x \parallel_W = \parallel Wx \parallel$ is a vector norm.\\

Exercise 3.2\\
(a) Let $|x_j| = \max\limits_{1 \leq i \leq m} |x_i|$\\
\centerline {$ \parallel x \parallel_2 = [\sum\limits_{i=1}^{m}|x_i|^2]^{1/2} = |x_j| + [\sum\limits_{i=1,i\neq j }^{m} |x_i|^2]^{1/2} \geq |x_j| = \max\limits_{1 \leq i \leq m} |x_i| = \parallel x \parallel_\infty $}\
Example for equality: $ x = \bcm 1\\0\\ \vdots \\0 \ecm \Rightarrow \parallel x \parallel_2 = \parallel x \parallel_\infty = 1$\\

(b) Let $|x_j| = \max\limits_{1 \leq i \leq m} |x_i|$\\
\centerline {$\parallel x \parallel_2 = [\sum\limits_{i=1}^{m}|x_i|^2]^{1/2} \leq [\sum\limits_{i=1}^{m}|x_j|^2]^{1/2} = [m|x_j|^2]^{1/2} = \sqrt{m}|x_j| = \sqrt{m}\parallel x \parallel_\infty$}\\
Example for equality: $ x = \bcm 1\\1\\ \vdots \\1 \ecm \Rightarrow \parallel x \parallel_2 = \sqrt{m}\parallel x \parallel_\infty = \sqrt{m}$\\

(c) From the solution of (a) and (b):\\
Let x be a n-column vector\\
\centerline {$\frac{\parallel Ax\parallel_\infty}{\parallel x\parallel_\infty} \leq \frac{\parallel Ax\parallel_2}{\parallel x\parallel_\infty} \leq \sqrt{n}\frac{\parallel Ax\parallel_\infty}{\parallel x\parallel_2}$}\\
Take the Supremum of both sides gives:\\
\centerline {$\sup\limits_{x \neq 0}\frac{\parallel Ax\parallel_\infty}{\parallel x\parallel_\infty} \leq \sup\limits_{x \neq 0}\frac{\sqrt{n}\parallel Ax\parallel_2}{\parallel x\parallel_2} = \sqrt{n}\sup\limits_{x \neq 0}\frac{\parallel Ax\parallel_2}{\parallel x\parallel_2}$}\\
which by definition means:\\
\centerline {$\parallel A\parallel_\infty \leq \sqrt{n}\parallel A\parallel_2$}\\
Example for equality: $ A = \bcm 1\\0\\ \vdots \\0 \ecm \Rightarrow \parallel A \parallel_\infty = \sqrt{1}\parallel A \parallel_2 = 1$\\

(d) From the solution of (a) and (b):\\
Let x be a n-column vector\\
Since $Ax$ is a m-column vector\\
\centerline {$\frac{\sqrt{m}\parallel Ax\parallel_\infty}{\parallel x\parallel_\infty} \geq \frac{\parallel Ax\parallel_2}{\parallel x\parallel_\infty} \geq \frac{\parallel Ax\parallel_2}{\parallel x\parallel_2}$}\\
Take the Supremum of both sides gives:\\
\centerline {$\sup\limits_{x \neq 0}\sqrt{m}\frac{\parallel Ax\parallel_\infty}{\parallel x\parallel_\infty}  = \sqrt{m}\sup\limits_{x \neq 0}\frac{\parallel Ax\parallel_\infty}{\parallel x\parallel_\infty}\geq\sup\limits_{x \neq 0}\frac{\parallel Ax\parallel_2}{\parallel x\parallel_2}$}\\
which by definition means:\\
\centerline {$\parallel A\parallel_2 \leq \sqrt{m}\parallel A\parallel_\infty$}\\
Example for equality: $ A = \bcm 1,0, \cdots ,0 \ecm \Rightarrow \parallel A \parallel_2 = \sqrt{1}\parallel A \parallel_\infty = 1$\\

Exercise 3.4\\
(a) Submatrix B can be obtained by $m \times n$ matrix A multiplied from left with an $m \times m$ identity matrix with only $\mu$ rows remaining and multiplied from right with an $n \times n$ identity matrix with only $\nu$ columns remaining. The rows indicated in the left multiplying matrix and the columns indicated in the right multiplying matrix will be the corresponding rows and columns of B from A.\\

(b)
Let the left multiplying matrix to be $C \in \mathbb{C}^{\mu \times m}$ and the right multiplying matrix to be $D \in \mathbb{C}^{n \times \nu}$.\\
\centerline {$\parallel B\parallel_p = \parallel CAD\parallel_p \leq \parallel C\parallel_p\parallel A\parallel_p\parallel D\parallel_p$}\\
Since matrix $C$ and $D$ are reduced from Identity matrix, $\parallel C\parallel_p \geq 1$ and  $\parallel D\parallel_p \geq 1 \tab \forall 1 \leq p \leq \infty$.\\
Therefore,\\
\centerline {$\parallel B\parallel_p \leq \parallel C\parallel_p\parallel A\parallel_p\parallel D\parallel_p \leq \parallel A\parallel_p \tab \forall 1 \leq p \leq \infty$}






%--------------------------------------------------------------------------
\vskip 1cm
\hrule
{\bf Problem 5.}

Linear algebra is widely used in image processing.  An
image that appears on a computer screen, for example, is typically
represented as an array of pixels that could be viewed as forming
a matrix.  If it's a color image then there are typically three
arrays (e.g. for the red, green, and blue channels ({\tt [R,G,B]}
to describe an arbitrary color) but for simplicity we will consinder
only a grayscale image, and assume that a single value ranges from
0 for white to 1 for black.  The array below might then represent
a $10 \times 10$ image with 100 pixels as shown in the following figure.

\begin{verbatim}
 0         0         0         0         0         0         0         0         0         0
 0         0         0         0         0         0         0         0         0         0
 0    0.3000    0.3000    0.3000    0.3000         0         0         0         0         0
 0    0.3000    0.3000    0.9000    0.9000    0.6000    0.6000    0.6000         0         0
 0    0.3000    0.3000    0.9000    0.9000    0.6000    0.6000    0.6000         0         0
 0         0         0    0.6000    0.6000    0.6000    0.6000    0.6000         0         0
 0         0         0    0.6000    0.6000    0.6000    0.6000    0.6000         0         0
 0         0         0    0.6000    0.6000    0.6000    0.6000    0.6000         0         0
 0         0         0    0.6000    0.6000    0.6000    0.6000    0.6000         0         0
 0         0         0         0         0         0         0         0         0         0
\end{verbatim}

\centerline{\includegraphics[width=3.5in]{image1.png}}

In Matlab, if $C$ is defined to be the $10 \times 10$ matrix above, then the
commands below will produce this plot (see the end of the assignment for a
Python version):

\begin{verbatim}
cmap = linspace(1,0,11)' * [1 1 1];   % define grayscale colormap
figure
colormap(cmap)     % use gray scale colormap
image(11*C)        % plot image (indexing into colormap of length 11)

hold on    % now add grid lines for clarity...
x = linspace(0.5,10.5,11);
y = x;
[X,Y] = meshgrid(x,y)
plot(X',Y','r')    % plot horizontal grid lines
plot(X,Y,'r')      % plot vertical grid lines
axis square        % scale x and y axis equally
hold off
\end{verbatim}

Note that we scale $C$ by 11 to index into the colormap which is defined here
as an $11 \times 3$ matrix with {\tt [R,G,B]} values corresponding to 11
shades of gray.  (Note that this matrix has been specified as an outer product
of a column vector with 11 components and a row vector with 3 components.
Print these things out if you are not sure what this matrix looks like.)

The matrix $C$ given above can also be specified fairly easily as a
combination of two outer products, $C = 0.3a_1b_1^T + 0.6a_2b_2^T$ where
for example $a_1 = [0,0,1,1,1,0,0,0,0,0]^T$ and $b_1 =
[0,1,1,1,1,0,0,0,0,0]^T$.

{\bf Problem 5a.} Compute the matrix $A_1 = a_1b_1^T$ and produce a plot of the image
that this describes.

Solution:\\
$A_1 = a_1b_1^T =$
$\bcm 0,0,0,0,0,0,0,0,0,0\\
0,0,0,0,0,0,0,0,0,0\\
0,1,1,1,1,0,0,0,0,0\\
0,1,1,1,1,0,0,0,0,0\\
0,1,1,1,1,0,0,0,0,0\\
0,0,0,0,0,0,0,0,0,0\\
0,0,0,0,0,0,0,0,0,0\\
0,0,0,0,0,0,0,0,0,0\\
0,0,0,0,0,0,0,0,0,0\\
0,0,0,0,0,0,0,0,0,0\ecm$\\
\centerline{\includegraphics[width=3.5in]{5a_A1.eps}}

{\bf Problem 5b.} Determine the vectors $a_2,~ b_2$ and the matrix $A_2 = a_2b_2^T$
so that the matrix above can be written as $A = 0.3A_1 + 0.6A_2$.

Solution:\\
$a_2 = \bcm 0,0,0,1,1,1,1,1,1,0\ecm ^T$ and $b_2 = \bcm 0,0,0,1,1,1,1,1,0,0\ecm ^T$\\
$A_2 = a_2b_2^T =$
$\bcm 0,0,0,0,0,0,0,0,0,0\\
0,0,0,0,0,0,0,0,0,0\\
0,0,0,0,0,0,0,0,0,0\\
0,0,0,0,0,0,0,0,0,0\\
0,0,0,1,1,1,1,1,0,0\\
0,0,0,1,1,1,1,1,0,0\\
0,0,0,1,1,1,1,1,0,0\\
0,0,0,1,1,1,1,1,0,0\\
0,0,0,1,1,1,1,1,0,0\\
0,0,0,0,0,0,0,0,0,0\ecm$\\
\centerline{\includegraphics[width=3.5in]{5b_A2.eps}}

Note that it takes much less storage to store the image in terms of the
vectors forming these outer products than it would to store every element of
the matrix.  This would be more dramatic if the image were say $1000 \times
1000$ (1 megapixel, with 1,000,000 matrix elements) rather than only
$10\times 10$.  This is a form of image compression.

This works so well because the matrix for this image is a rank 2
matrix and so only 2 outer products are needed.  If it were a full
rank matrix then this compression might not work so well. But images
generally aren't random pixels.  Instead they have a lot of structure
and so it is sometimes possible to approximate an image well with
a low rank approximation.

The singular value decomposition gives a way of writing any matrix
as a linear combination of rank one matrices (outer products) and if the
singular values decay rapidly (which they often do for real images) then we
may be able to truncate the SVD to relatively few terms.

{\bf Problem 5c.} Use Matlab or Python to determine the SVD of the matrix $A$ above
and confirm that only two singular values are nonzero.  With this
decomposition we see that we can express $C$ as $C = \sigma_1 u_1v_1^* +
\sigma_2 u_2 v_2^*$.  Confirm that this is true and print out the singular
values and vectors used in this sum, and also the two
rank-1 matrices that appear in this sum separately.\\
Solution:\\
Let $\bcm U,\Sigma,V^*\ecm = svd[C]$ then:\\
$\sigma1 = 3.5708,~ \sigma2 = 0.7544$ and all other singular values are zero in matrix $\Sigma$\\
Also \\
\centerline {$u_1 = [0~0~-0.0976~-0.4688~-0.4688~-0.3711~-0.3711~-0.3711~-0.3711~0]^T$}\\
\centerline {$u_2 = [0~0~-0.6473~-0.3707~-0.3707~ 0.2766~0.2766~0.2766~0.2766~0]^T$}\\
\centerline {$v_1^* = [0~-0.0870~-0.0870~-0.4939~-0.4939~-0.4070~-0.4070~-0.4070~0~0]^T$}\\
\centerline {$v_2^* = [0~-0.5522~-0.5522~-0.2618~ -0.2618~0.2905~0.2905~0.2905~0~0]^T$}\\

$\sigma_1u_1v_1^* = $
$\bcm 0,0,0,0,0,0,0,0,0,0\\
0,0,0,0,0,0,0,0,0,0\\
0,0.0303,0.0303,0.1722,0.1722,0.1419,0.1419,0.1419,0,0\\
0,0.1456,0.1456,0.8268,0.8268,0.6812,0.6812,0.6812,0,0\\
0,0.1456,0.1456,0.8268,0.8268,0.6812,0.6812,0.6812,0,0\\
0,0.1153,0.1153,0.6546,0.6546,0.5394,0.5394,0.5394,0,0\\
0,0.1153,0.1153,0.6546,0.6546,0.5394,0.5394,0.5394,0,0\\
0,0.1153,0.1153,0.6546,0.6546,0.5394,0.5394,0.5394,0,0\\
0,0.1153,0.1153,0.6546,0.6546,0.5394,0.5394,0.5394,0,0\\
0,0,0,0,0,0,0,0,0,0\ecm$\\

$\sigma_2u_2v_2^* = $
$\bcm 0,0,0,0,0,0,0,0,0,0\\
0,0,0,0,0,0,0,0,0,0\\
0,0.2697,0.2697,0.1278,0.1278,-0.1419,-0.1419,-0.1419,0,0\\
0,0.1544,0.1544,0.0732,0.0732,-0.0812,-0.0812,-0.0812,0,0\\
0,0.1544,0.1544,0.0732,0.0732,-0.0812,-0.0812,-0.0812,0,0\\
0,-0.1153,-0.1153,-0.0546,-0.0546,0.0606,0.0606,0.0606,0,0\\
0,-0.1153,-0.1153,-0.0546,-0.0546,0.0606,0.0606,0.0606,0,0\\
0,-0.1153,-0.1153,-0.0546,-0.0546,0.0606,0.0606,0.0606,0,0\\
0,-0.1153,-0.1153,-0.0546,-0.0546,0.0606,0.0606,0.0606,0,0\\
0,0,0,0,0,0,0,0,0,0\ecm$\\

{\bf Problem 5d.} Plot the image given by the matrix $\sigma_1 u_1v_1^*$.  This is
the best approximation to the matrix $C$ by a rank-1 matrix.\\
Solution:\\
\centerline{\includegraphics[width=3.5in]{5d.eps}}

{\bf Problem 5e.} Confirm that $u_1$ and $u_2$ are orthonormal vectors
(whereas $a_1$ and $a_2$ were not).  In fact $u_1$ and $u_2$ form an
orthonormal basis for the 2-dimensional space $\text{span}(a_1,a_2)$.
To check this: In Matlab (or Python), form the matrix
with columns $[u_1 | u_2 | a_1 | a_2]$ and confirm that it has rank 2.
Explain why this shows that the vectors $u_1$ and $u_2$ span the same
subspace of $\complex^2$ as that spanned by $a_1$ and $a_2$.\\

Solution:\\
Inner product of $u_1$ and $u_2$ is $u_1^Tu_2 = 0$ and $\parallel u_1\parallel = \parallel u_2\parallel = 1 ~ \Rightarrow u_1$ and $u_2$ are orthonormal.\\
Since $a_1$ and $a_2$ are linearly independent:\\
\centerline {$span(u_1,~u_2) = span(a_1,~a_2) = \mathbb {C}^2$}

\vskip 5pt
{\bf Note:} The SVD is not the best approach for image compression,
and the point of this example is primarily to get familiar with the
SVD and the idea of low-rank representations of a matrix.

See
\url{https://www.mathworks.com/company/newsletters/articles/professor-svd.html}
for an introductory article by Cleve Moler (inventor of Matlab) on
the SVD that also shows an example of using it to compress an image
of Gene Golub, who did much to popularize the use of the SVD for a
wide range of applications.

\vskip 1cm
{\bf Plotting an image in Python:}

This produces a similar plot to the one shown above, but flipped vertically
and also recall that Python indexing starts at 0.

\begin{verbatim}

imshow(C, cmap='Greys', interpolation='none')
x = linspace(-0.5,9.5,11)
y = x
[X,Y] = meshgrid(x,y)
plot(X.T,Y.T,'r');
plot(X,Y,'r');
axis('scaled')
axis([-0.5, 9.5, -0.5, 9.5])
\end{verbatim}

\end{document}

